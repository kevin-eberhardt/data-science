{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# God Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#1. load environment variables and data\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#add working directory to sys path to execute utils/dataset.py\n",
    "working_dir = os.environ.get(\"WORKING_DIRECTORY\")\n",
    "sys.path.insert(0, working_dir)\n",
    "\n",
    "from utils.dataset import get_data \n",
    "\n",
    "from utils.pipeline_moduls import fs_colinearity, fs_vif, outlier_label, outlier_num, dim_reduction\n",
    "\n",
    "df = get_data()\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "#drop 'quality' from numerical features (its a series)\n",
    "numerical_features = numerical_features.drop('quality')\n",
    "label = pd.Series('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection_label = FunctionTransformer(outlier_label).set_output(transform=\"pandas\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection = FunctionTransformer(outlier_num).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def feature_selection(df,colinearity_threshold=0.5, correlation_threshold=0.08, vif_threshold=5):\n",
    "    dropped_features = []\n",
    "    dropped_features_set = set(dropped_features)\n",
    "\n",
    "    # Add elements from fs_colinearity to dropped_features_set\n",
    "    dropped_features_set.update(fs_colinearity(df, colinearity_threshold, correlation_threshold))\n",
    "\n",
    "    # Add elements from fs_vif to dropped_features_set\n",
    "    dropped_features_set.update(fs_vif(df, correlation_threshold, vif_threshold))\n",
    "\n",
    "    # Convert dropped_features_set back to a list\n",
    "    dropped_features = list(dropped_features_set)\n",
    "    print(\"Dropping Features: \", dropped_features)\n",
    "    # Drop the features in dropped_features from the DataFrame\n",
    "    df = df.drop(columns=dropped_features)\n",
    "\n",
    "    # Save dropped features list to a JSON file\n",
    "    with open('dropped_features.json', 'w') as f:\n",
    "        json.dump(dropped_features, f)\n",
    "    return df\n",
    "feature_selection = FunctionTransformer(feature_selection).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline = Pipeline(steps=[\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Pipeline: Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\").set_output(transform=\"pandas\")\n",
    "\n",
    "#pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[])\n",
    "categorical_pipeline.steps.append(('imputer', categorical_imputer))\n",
    "categorical_pipeline.steps.append(('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform=\"pandas\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Pipeline: Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for numerical features\n",
    "numeric_pipeline = Pipeline(steps=[])\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\").set_output(transform=\"pandas\")\n",
    "\n",
    "numeric_pipeline.steps.append(('imputer', numerical_imputer))\n",
    "numeric_pipeline.steps.append(('outlier_detection', outlier_detection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Pipeline: Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for label\n",
    "label_pipeline = Pipeline(steps=[])\n",
    "label_pipeline.steps.append(('imputer', numerical_imputer))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('label', label_pipeline, label)\n",
    "    ]).set_output(transform=\"pandas\")\n",
    "cleaning_pipeline.steps.append(('preprocessor', preprocessor))\n",
    "cleaning_pipeline.steps.append((\"outlier_detection_label\", outlier_detection_label))\n",
    "cleaning_pipeline.steps.append(('feature_selection', feature_selection))\n",
    "cleaning_pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"estimator\": LinearRegression(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"fit_intercept\": [True, False],\n",
    "                \"copy_X\": [True, False],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTreeRegressor\",\n",
    "        \"estimator\": DecisionTreeRegressor(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"criterion\": [\"squared_error\"],\n",
    "                \"splitter\": [\"best\", \"random\"],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 5, 10]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForestRegressor\",\n",
    "        \"estimator\": RandomForestRegressor(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"criterion\": [\"squared_error\"],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 5, 10],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gradient Boosting Regressor\",\n",
    "        \"estimator\": GradientBoostingRegressor(),\n",
    "        \"hyperparameters\":\n",
    "        {\n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"learning_rate\": [0.01, 0.011, 0.012],\n",
    "                \"loss\": [\"squared_error\"],\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "best_models_runtime = []\n",
    "\n",
    "def god_function(dirty_df):\n",
    "    for model in models:\n",
    "        print(model[\"name\"])\n",
    "        print(\"-\"*len(model[\"name\"]))\n",
    "        clean_df = pd.DataFrame(cleaning_pipeline.fit_transform(dirty_df))\n",
    "        X = clean_df.drop('label__quality', axis=1)\n",
    "        y = clean_df['label__quality']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "        grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "        grid = grid.fit(X_train, y_train)\n",
    "        print(\"Best Parameters:\")\n",
    "        print(grid.best_params_)\n",
    "        print(\"\")\n",
    "        print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "        print(\"Fit Time:\", grid.refit_time_)\n",
    "        print(\"\")\n",
    "        #save best estimator from grid with pickle\n",
    "        with open(\"./models/\" + model[\"name\"] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(grid.best_estimator_, f)\n",
    "        \n",
    "        #save best model runtime\n",
    "        best_models_runtime.append({\n",
    "            \"name\": model[\"name\"], \n",
    "            \"runtime\": grid.refit_time_,\n",
    "            \"best_score\": grid.best_score_,\n",
    "            \"best_params\": grid.best_params_\n",
    "        })\n",
    "\n",
    "god_function(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significant_features(X, y, model):\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "\n",
    "    residuals = y - model.predict(X)\n",
    "\n",
    "    n = len(y)\n",
    "    p = X.shape[1]\n",
    "    df = n - p - 1\n",
    "\n",
    "    mse = np.sum(residuals ** 2) / df\n",
    "    variance_covariance_matrix = mse * np.linalg.inv(np.dot(X.T, X))\n",
    "    standard_errors = np.sqrt(np.diagonal(variance_covariance_matrix))\n",
    "\n",
    "\n",
    "    t_values = coefficients / standard_errors\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df))\n",
    "\n",
    "    headers = ['Feature', 'Coefficient', 'Standard Error', 't-value', 'p-value']\n",
    "\n",
    "    prediction_metrics = pd.DataFrame(columns=headers)\n",
    "    for i in range(len(coefficients)):\n",
    "        prediction_metrics.loc[i] = [X.columns.values[i], coefficients[i], standard_errors[i], t_values[i], p_values[i]]\n",
    "\n",
    "    #remove rows with p-value > 0.05\n",
    "    features_to_remove = prediction_metrics[prediction_metrics['p-value'] > 0.05]['Feature'].values\n",
    "    print(\"Removing features: \", features_to_remove)\n",
    "    prediction_metrics = prediction_metrics[prediction_metrics['p-value'] < 0.05]\n",
    "    return prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = [{\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"estimator\": LinearRegression(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"fit_intercept\": [True, False],\n",
    "                \"copy_X\": [True, False],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Support Vector Machine\",\n",
    "        \"estimator\": SVR(),\n",
    "        \"hyperparameters\":\n",
    "        {\n",
    "            \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"degree\": [1, 2, 3, 4, 5],\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 1000],\n",
    "            \"epsilon\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    }\n",
    "    }\n",
    "    ]\n",
    "dirty_df = df.copy(deep=True)\n",
    "for model in test_models:\n",
    "    print(model[\"name\"])\n",
    "    print(\"-\"*len(model[\"name\"]))\n",
    "    pipeline = cleaning_pipeline\n",
    "    #pipeline.steps.pop(2)\n",
    "    clean_df = pd.DataFrame(pipeline.fit_transform(dirty_df))\n",
    "    X = clean_df.drop('label__quality', axis=1)\n",
    "    y = clean_df['label__quality']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "    grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "    print(\"Fit Time:\", grid.refit_time_)\n",
    "    print(\"\")\n",
    "    best_model = grid.best_estimator_\n",
    "    significant_features = calculate_significant_features(X_train, y_train, best_model)\n",
    "    #keep columns of X only if they are present in significant_features\n",
    "    X_train = X_train[significant_features['Feature'].values]\n",
    "    X_test = X_test[significant_features['Feature'].values]\n",
    "    grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "    print(\"Fit Time:\", grid.refit_time_)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.score()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END TESTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Meth-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model with pickle\n",
    "best_model = pickle.load(open('best_model__random_forest.pkl','rb'))\n",
    "#select randomly 1000 data points from df and drop selected ones\n",
    "df_validation = df.sample(n=1000, random_state=1)\n",
    "clean_df = df.drop(df_validation.index)\n",
    "\n",
    "X = clean_df.drop('label__quality', axis=1)\n",
    "y = clean_df['label__quality']\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=200, stratify=y)\n",
    "\n",
    "print(best_model.score(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df.sample(n=1000, random_state=42)\n",
    "\n",
    "clean_df = df.drop(df_validation.index)\n",
    "clean_df = pd.DataFrame(cleaning_pipeline.fit_transform(clean_df))\n",
    "\n",
    "X_clean = clean_df.drop('label__quality', axis=1)\n",
    "y_clean = clean_df['label__quality']\n",
    "\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, y_clean, test_size=0.2, random_state=200, stratify=y_clean)\n",
    "best_model = RandomForestRegressor(criterion = 'squared_error', max_depth = None, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200)\n",
    "best_model = best_model.fit(X_train_clean, y_train_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.score(X_test_clean, y_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.DataFrame(cleaning_pipeline.fit_transform(df_validation))\n",
    "X_validation = df_validation.drop('label__quality', axis=1)\n",
    "y_validation = df_validation['label__quality']\n",
    "\n",
    "best_model.score(X_validation, y_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
