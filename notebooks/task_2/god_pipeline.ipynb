{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science Task 2\n",
    "Data preprocessing and modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from wines: 8000it [00:00, 22709.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wine type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>minerals</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pinot noir</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.1</td>\n",
       "      <td>76.729301</td>\n",
       "      <td>894.94</td>\n",
       "      <td>186.639301</td>\n",
       "      <td>109.91</td>\n",
       "      <td>0.048</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.32</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.795712</td>\n",
       "      <td>1160.95</td>\n",
       "      <td>251.875712</td>\n",
       "      <td>247.08</td>\n",
       "      <td>0.039</td>\n",
       "      <td>15.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.99163</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.5</td>\n",
       "      <td>85.193710</td>\n",
       "      <td>789.82</td>\n",
       "      <td>304.703710</td>\n",
       "      <td>219.51</td>\n",
       "      <td>0.035</td>\n",
       "      <td>45.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.98949</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>12.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.27</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.976525</td>\n",
       "      <td>777.86</td>\n",
       "      <td>237.586525</td>\n",
       "      <td>225.61</td>\n",
       "      <td>0.045</td>\n",
       "      <td>48.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1.00014</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.19</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.599673</td>\n",
       "      <td>785.72</td>\n",
       "      <td>95.399673</td>\n",
       "      <td>89.80</td>\n",
       "      <td>0.041</td>\n",
       "      <td>62.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.99508</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.37</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.3</td>\n",
       "      <td>22.403749</td>\n",
       "      <td>1044.95</td>\n",
       "      <td>289.523749</td>\n",
       "      <td>267.12</td>\n",
       "      <td>0.057</td>\n",
       "      <td>25.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.99480</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.5</td>\n",
       "      <td>23.875866</td>\n",
       "      <td>888.61</td>\n",
       "      <td>133.545866</td>\n",
       "      <td>109.67</td>\n",
       "      <td>0.047</td>\n",
       "      <td>20.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.99178</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.48</td>\n",
       "      <td>11.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.7</td>\n",
       "      <td>23.309699</td>\n",
       "      <td>1381.79</td>\n",
       "      <td>266.529699</td>\n",
       "      <td>243.22</td>\n",
       "      <td>0.052</td>\n",
       "      <td>56.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.99398</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4.7</td>\n",
       "      <td>49.165745</td>\n",
       "      <td>1456.41</td>\n",
       "      <td>269.915745</td>\n",
       "      <td>220.75</td>\n",
       "      <td>0.046</td>\n",
       "      <td>57.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gamay</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>8.9</td>\n",
       "      <td>54.450579</td>\n",
       "      <td>929.44</td>\n",
       "      <td>377.690579</td>\n",
       "      <td>323.24</td>\n",
       "      <td>0.036</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.99350</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wine type  fixed acidity  volatile acidity  citric acid  \\\n",
       "0          Pinot noir            5.8              0.15         0.49   \n",
       "1              Merlot            6.6              0.25         0.32   \n",
       "2          Chardonnay            6.7              0.21         0.34   \n",
       "3              Merlot            8.3              0.28         0.27   \n",
       "4              Merlot            7.5              0.42         0.19   \n",
       "5              Merlot            7.3              0.34         0.30   \n",
       "6              Merlot            7.6              0.21         0.49   \n",
       "7          Chardonnay            6.0              0.25         0.40   \n",
       "8  Cabernet Sauvignon            6.7              0.18         0.19   \n",
       "9               Gamay            7.7              0.28         0.39   \n",
       "\n",
       "   residual sugar  magnesium  flavanoids    minerals  calcium  chlorides  \\\n",
       "0             1.1  76.729301      894.94  186.639301   109.91      0.048   \n",
       "1             5.6   4.795712     1160.95  251.875712   247.08      0.039   \n",
       "2             1.5  85.193710      789.82  304.703710   219.51      0.035   \n",
       "3            17.5  11.976525      777.86  237.586525   225.61      0.045   \n",
       "4             6.9   5.599673      785.72   95.399673    89.80      0.041   \n",
       "5             1.3  22.403749     1044.95  289.523749   267.12      0.057   \n",
       "6             2.5  23.875866      888.61  133.545866   109.67      0.047   \n",
       "7             5.7  23.309699     1381.79  266.529699   243.22      0.052   \n",
       "8             4.7  49.165745     1456.41  269.915745   220.75      0.046   \n",
       "9             8.9  54.450579      929.44  377.690579   323.24      0.036   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 21.0                  98.0  0.99290  3.19       0.48   \n",
       "1                 15.0                  68.0  0.99163  2.96       0.52   \n",
       "2                 45.0                 123.0  0.98949  3.24       0.36   \n",
       "3                 48.0                 253.0  1.00014  3.02       0.56   \n",
       "4                 62.0                 150.0  0.99508  3.23       0.37   \n",
       "5                 25.0                 173.0  0.99480  3.26       0.51   \n",
       "6                 20.0                 130.0  0.99178  3.15       0.48   \n",
       "7                 56.0                 152.0  0.99398  3.16       0.88   \n",
       "8                 57.0                 161.0  0.99460  3.32       0.66   \n",
       "9                  8.0                 117.0  0.99350  3.06       0.38   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.2        5  \n",
       "1     11.1        6  \n",
       "2     12.6        7  \n",
       "3      9.1        6  \n",
       "4     10.0        6  \n",
       "5      9.1        6  \n",
       "6     11.1        5  \n",
       "7     10.5        6  \n",
       "8     10.5        6  \n",
       "9     12.0        2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import warnings\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#add working directory to sys path to execute utils/dataset.py\n",
    "working_dir = os.environ.get(\"WORKING_DIRECTORY\")\n",
    "sys.path.insert(0, working_dir)\n",
    "\n",
    "from utils.dataset import get_data \n",
    "from utils.pipeline_moduls import fs_colinearity, fs_vif, outlier_label, outlier_num, dim_reduction\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = get_data()\n",
    "\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing Pipeline Setup\n",
    "Pipeline for missing value imputation, outlier detection and imputation, feature scaling, feature encoding, feature selection and label outlier removal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Transform custom functions into pipeline steps\n",
    "we define custom functions to handle the data and transform them into pipeline steps with the FunctionTransformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Label Outlier Detection\n",
    "Function that removes outliers from the label (quality > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detection_label = FunctionTransformer(outlier_label).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Feature Outlier Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using IQR to detect outliers in the features and impute with best method (mean or median)\n",
    "- median for normal distributed features, mean for skewed and uniform distributed features\n",
    "- we checked the impact of using different approaches to impute the outliers and found that the impact is negligible on the prediction results (tested for decision tree and linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "outlier_detection = FunctionTransformer(outlier_num).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Scaling the numeric features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scaling is useful and necessary for following models: \n",
    "- KNN\n",
    "- ANN\n",
    "- SVM\n",
    "\n",
    "Scaling is not useful or improves results significantly for following models:\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- Linear Regression\n",
    "\n",
    "\n",
    "#### Results without scaling vs with best scaling method:\n",
    "| Model | MSE | R² |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Random Forest | -1.46% | +1.1% |\n",
    "| Gradient Boosting | +4.8% | -2.9% |\n",
    "| Linear Regression | -0.5% | +1.5% |\n",
    "| KNN* | -∞ | +∞ |\n",
    "| ANN* | -∞ | +∞ |\n",
    "| SVM* | -∞ | +∞ |\n",
    "\n",
    "\n",
    "*all had Error > 1 and R² < 0\n",
    "\n",
    "\\\n",
    "The different scaling methods have no significant impact on the results for most models. \n",
    "\n",
    "\n",
    "But for KNN, MinMaxScaler outperformed the other scalers.\n",
    "\n",
    "#### MSE:\n",
    "```\n",
    "MinMaxScaler:   0%\n",
    "StandardScaler: +74.5%\n",
    "RobustScaler:   +62,1%\n",
    "```\n",
    "#### R²:\n",
    "```\n",
    "MinMaxScaler:   0%\n",
    "StandardScaler: -68.4%\n",
    "RobustScaler:   -52.6%\n",
    "```\n",
    "**MinMaxScaler** is reliable and fast, so it is the best choice for scaling.\n",
    "\n",
    "SVM's without scaling the data are not usable, because the training takes too long.\n",
    "\n",
    "#### Polinomial Feature Transformation\n",
    "Polinomial feature transformation did not improve the results.\n",
    "Deviation to best result:\\\n",
    "`+1.16% MSE (Random Forest)`\\\n",
    "`-1.24% R² (Random Forest)`\n",
    "\n",
    "#### Dimensionality Reduction (PCA) \n",
    "PCA did not improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scaler_minmax = MinMaxScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Identification of collinear features (fs_colinearity) based on a threshold for the collinearity (colinearity_threshold).\n",
    "- Identify features with high multicollinearity (fs_vif) based on a threshold for the variance inflation factor (VIF) (vif_threshold).  \n",
    "- Saving the features in dropped_features identified by the two functions below if they are under the correlation threshold (correlation_threshold).\n",
    "\n",
    "```python\n",
    "def fs_colinearity(df, colinearity_threshold=0.5,correlation_threshold=0.1):\n",
    "    ...\n",
    "    return dropped_features\n",
    "\n",
    "\n",
    "def fs_vif(df, correlation_threshold=0.1, vif_threshold=5):\n",
    "    ...\n",
    "    return dropped_features\n",
    " \n",
    "```\n",
    "- Saving the list of dropped features in a JSON file named \"dropped_features.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def feature_selection(df,colinearity_threshold=0.5, correlation_threshold=0.1, vif_threshold=5):\n",
    "    dropped_features = []\n",
    "    dropped_features_set = set(dropped_features)\n",
    "\n",
    "    # Add elements from fs_colinearity to dropped_features_set\n",
    "    dropped_features_set.update(fs_colinearity(df, colinearity_threshold, correlation_threshold))\n",
    "\n",
    "    # Add elements from fs_vif to dropped_features_set\n",
    "    dropped_features_set.update(fs_vif(df, correlation_threshold, vif_threshold))\n",
    "\n",
    "    # Convert dropped_features_set back to a list\n",
    "    dropped_features = list(dropped_features_set)\n",
    "    print(\"Dropping Features: \", dropped_features)\n",
    "    # Drop the features in dropped_features from the DataFrame\n",
    "    df = df.drop(columns=dropped_features)\n",
    "\n",
    "    # Save dropped features list to a JSON file\n",
    "    if df.shape[0] > 7000:\n",
    "        with open('dropped_features.json', 'w') as f:\n",
    "            json.dump(dropped_features, f)\n",
    "    else:\n",
    "        with open('./models/7000_samples/dropped_features.json', 'w') as f:\n",
    "            json.dump(dropped_features, f)\n",
    "\n",
    "    return df\n",
    "feature_selection = FunctionTransformer(feature_selection).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipeline Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For the creation of the pipeline, so-called sub-pipelines are used to make a clean separation between numeric and categorical features as well as labels. \n",
    "- Missing values should always be imputed using the average (for numeric features) or the most frequent value (for categorical features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cleaning_pipeline = Pipeline(steps=[\n",
    "])\n",
    "\n",
    "cleaning_pipeline_scaled = Pipeline(steps=[\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sub-Pipeline: Categorical Features\n",
    "this sub-pipeline handles the categorical feature \"wine type\" \n",
    "\n",
    "we use one-hot encoding to transform the categorical feature into binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\").set_output(transform=\"pandas\")\n",
    "\n",
    "#pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[])\n",
    "categorical_pipeline.steps.append(('imputer', categorical_imputer))\n",
    "categorical_pipeline.steps.append(('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform=\"pandas\")))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sub-Pipeline: Numerical Features\n",
    "this sub-pipeline handles the numerical features like \"fixed acidity\", \"volatile acidity\" or \"citric acid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#pipeline for numerical features\n",
    "numeric_pipeline = Pipeline(steps=[])\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\").set_output(transform=\"pandas\")\n",
    "\n",
    "numeric_pipeline.steps.append(('imputer', numerical_imputer))\n",
    "numeric_pipeline.steps.append(('outlier_detection', outlier_detection))\n",
    "\n",
    "#pipeline_scaled for numerical features\n",
    "numeric_pipeline_scaled = Pipeline(steps=[])\n",
    "\n",
    "numeric_pipeline_scaled.steps.append(('imputer', numerical_imputer))\n",
    "numeric_pipeline_scaled.steps.append(('outlier_detection', outlier_detection))\n",
    "numeric_pipeline_scaled.steps.append(('scaler', scaler_minmax))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sub-Pipeline: Label\n",
    "this pipeline handles the label \"quality\" by just passing it through (label outlier detections happens later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#pipeline for label\n",
    "label_pipeline = Pipeline(steps=[])\n",
    "#generate pass through function\n",
    "pass_through = FunctionTransformer().set_output(transform=\"pandas\")\n",
    "label_pipeline.steps.append(('do_nothing', pass_through))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building the god pipeline (combining all steps)\n",
    "this step builds the different column transformers (pipeline branches) and combines them into one pipeline\n",
    "\n",
    "after that it adds the label outlier detection step and the feature selection step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Advantages of our approach\n",
    "- Clean separation between numeric, categorical and label columns.\n",
    "- This allows effective data cleaning and preprocessing to handle missing values, perform scaling, or code categorical features\n",
    "- Outlier detection is designed to help identify and, if necessary, remove outliers in the target variables to improve model performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "#drop 'quality' from numerical features (its a series)\n",
    "numerical_features = numerical_features.drop('quality')\n",
    "label = pd.Series('quality')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combining all steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Pipeline Setup (non-scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('label', label_pipeline, label)\n",
    "    ]).set_output(transform=\"pandas\")\n",
    "cleaning_pipeline.steps.append(('preprocessor', preprocessor))\n",
    "cleaning_pipeline.steps.append((\"outlier_detection_label\", outlier_detection_label))\n",
    "cleaning_pipeline.steps.append(('feature_selection', feature_selection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://github.com/kevin-eberhardt/data-science/blob/4b5a1c8d57100ab2db31598532192e3ea6502756/figures/cleaning_pipeline.png?raw=true\" alt=\"drawing\" width=\"100%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Pipeline Setup (scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline_scaled, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('label', label_pipeline, label)\n",
    "    ]).set_output(transform=\"pandas\")\n",
    "cleaning_pipeline_scaled.steps.append(('preprocessor', preprocessor_scaled))\n",
    "cleaning_pipeline_scaled.steps.append((\"outlier_detection_label\", outlier_detection_label))\n",
    "cleaning_pipeline_scaled.steps.append(('feature_selection', feature_selection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://github.com/kevin-eberhardt/data-science/blob/4b5a1c8d57100ab2db31598532192e3ea6502756/figures/cleaning_pipeline_scaled.png?raw=true\" alt=\"drawing\" width=\"100%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Hyperparamter\n",
    "\n",
    "- The use of hyperparameters is intended to adapt the models to the specific requirements of the data\n",
    "- The goal is to optimize model performance \n",
    "- In addition, the choice of the right hyperparameters should avoid overfitting or underfitting by achieving an appropriate balance between variance and bias.\n",
    "- this is done by using the Grid Search approach\n",
    "- we also use cross validation to avoid overfitting (GridSearchCV function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"estimator\": LinearRegression(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"fit_intercept\": [True, False],\n",
    "                \"copy_X\": [True, False]\n",
    "            },\n",
    "        \"scalable\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTreeRegressor\",\n",
    "        \"estimator\": DecisionTreeRegressor(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"criterion\": [\"squared_error\", \"friedman_mse\"],\n",
    "                \"splitter\": [\"best\", \"random\"],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 5, 10]\n",
    "            },\n",
    "        \"scalable\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForestRegressor\",\n",
    "        \"estimator\": RandomForestRegressor(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"criterion\": [\"squared_error\", \"friedman_mse\"],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 5, 10]\n",
    "            },\n",
    "        \"scalable\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gradient Boosting Regressor\",\n",
    "        \"estimator\": GradientBoostingRegressor(),\n",
    "        \"hyperparameters\":\n",
    "        {       \n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"max_depth\": [None, 3, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"learning_rate\": [0.1, 0.05, 0.001],\n",
    "                \"loss\": ['squared_error', 'absolute_error', 'huber'],\n",
    "        },\n",
    "        \"scalable\": 0\n",
    "    }, \n",
    "     {\n",
    "        \"name\": \"Support Vector Machine\",\n",
    "        \"estimator\": SVR(),\n",
    "        \"hyperparameters\": {\n",
    "            \"C\": [1, 10, 100],\n",
    "            \"kernel\": [\"rbf\", \"linear\", \"poly\"]\n",
    "        },\n",
    "        \"best_score\": 0.5567442927702857,\n",
    "        \"scalable\": 1\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ANN\",\n",
    "        \"estimator\": MLPRegressor(),\n",
    "        \"hyperparameters\": {\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'hidden_layer_sizes': [(100, ),(100, 50), (100, 50, 25), (100, 75, 50, 25)],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'solver': ['adam', 'lbfgs']\n",
    "        },\n",
    "        \"best_score\": 0.5567442927702857,\n",
    "        \"scalable\": 1\n",
    "    }, \n",
    "    {\n",
    "        \"name\": \"KNN\",\n",
    "        \"estimator\": KNeighborsRegressor(),\n",
    "        \"hyperparameters\": {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "            'weights' : ['uniform', 'distance']\n",
    "        },\n",
    "        \"best_score\": 0.5567442927702857,\n",
    "        \"scalable\": 1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Training with Grid Search CV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A cleaned version of the input dataset is created by applying the cleaning_pipeline and cleaning_pipeline_scaled\n",
    "- The cleaned dataset is split into training and test data (train_test_split) with a test size of 20%\n",
    "- A GridSearch is performed to find the best hyperparameters for the model by trying different combinations of the hyperparameters\n",
    "- The best parameters (grid.best_params_), best score (grid.best_score_), and test score (grid.score(X_test, y_test)) are outputted\n",
    "- The best models are written to a JSON file (best_models.json)\n",
    "- The goal here is to automate model training, hyperparameter optimization and scoring\n",
    "- The use of GridSearch allows to find the best hyperparameters for each model and to get an objective evaluation of the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "----------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'copy_X': True, 'fit_intercept': True}\n",
      "\n",
      "Best Score: 0.5343677291345037 \t Test Score: 0.5349169536244286\n",
      "Fit Time: 0.01699519157409668\n",
      "\n",
      "DecisionTreeRegressor\n",
      "---------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\n",
      "Best Score: 0.8108559058850483 \t Test Score: 0.8826506746325857\n",
      "Fit Time: 0.07397246360778809\n",
      "\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Best Score: 0.8637427441196293 \t Test Score: 0.897082006265386\n",
      "Fit Time: 15.103227853775024\n",
      "\n",
      "Gradient Boosting Regressor\n",
      "---------------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'learning_rate': 0.1, 'loss': 'huber', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "\n",
      "Best Score: 0.899523999581552 \t Test Score: 0.936529800366548\n",
      "Fit Time: 113.29849362373352\n",
      "\n",
      "Support Vector Machine\n",
      "----------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'C': 100, 'kernel': 'rbf'}\n",
      "\n",
      "Best Score: 0.6584843138065475 \t Test Score: 0.6814865291498295\n",
      "Fit Time: 56.38434290885925\n",
      "\n",
      "ANN\n",
      "---\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100, 75, 50, 25), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "\n",
      "Best Score: 0.6268604939101232 \t Test Score: 0.621348775200474\n",
      "Fit Time: 46.057313680648804\n",
      "\n",
      "KNN\n",
      "---\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'n_neighbors': 15, 'weights': 'distance'}\n",
      "\n",
      "Best Score: 0.8526570085018159 \t Test Score: 0.9177228815134711\n",
      "Fit Time: 0.038987159729003906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "best_models = []\n",
    "\n",
    "def god_function(dirty_df):\n",
    "    for model in models:\n",
    "        print(model[\"name\"])\n",
    "        print(\"-\"*len(model[\"name\"]))\n",
    "        if model[\"scalable\"] is not None:\n",
    "            if model[\"scalable\"] == 0:\n",
    "                clean_df = pd.DataFrame(cleaning_pipeline.fit_transform(dirty_df))\n",
    "            if model[\"scalable\"] == 1:\n",
    "                clean_df = pd.DataFrame(cleaning_pipeline_scaled.fit_transform(dirty_df))\n",
    "        X = clean_df.drop('label__quality', axis=1)\n",
    "        y = clean_df['label__quality']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "        grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=10, n_jobs=-1)\n",
    "        grid = grid.fit(X_train, y_train)\n",
    "        print(\"Best Parameters:\")\n",
    "        print(grid.best_params_)\n",
    "        print(\"\")\n",
    "        print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "        print(\"Fit Time:\", grid.refit_time_)\n",
    "        print(\"\")\n",
    "        m = {\n",
    "            \"name\": model[\"name\"],\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"best_score\": grid.best_score_,\n",
    "            \"fit_time\": grid.refit_time_,\n",
    "            \"test_score\":  grid.score(X_test, y_test)\n",
    "        }\n",
    "        best_models.append(m)\n",
    "        \n",
    "        #save best models to json\n",
    "        with open(\"./models/best_models.json\", \"w\") as f:\n",
    "            json.dump(best_models, f, indent=4)\n",
    "            \n",
    "        #save best estimator from grid with pickle\n",
    "        with open(\"./models/\" + model[\"name\"] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(grid.best_estimator_, f)\n",
    "        \n",
    "god_function(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Training with 7000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "----------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'copy_X': True, 'fit_intercept': True}\n",
      "\n",
      "Best Score: 0.536606014393309 \t Test Score: 0.5304305139944511\n",
      "Fit Time: 0.010557174682617188\n",
      "\n",
      "DecisionTreeRegressor\n",
      "---------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\n",
      "Best Score: 0.7406565554405997 \t Test Score: 0.7837270590236824\n",
      "Fit Time: 0.06697344779968262\n",
      "\n",
      "RandomForestRegressor\n",
      "---------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'criterion': 'friedman_mse', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Best Score: 0.8393837849617622 \t Test Score: 0.8606282958201987\n",
      "Fit Time: 13.033994674682617\n",
      "\n",
      "Gradient Boosting Regressor\n",
      "---------------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'learning_rate': 0.05, 'loss': 'absolute_error', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "\n",
      "Best Score: 0.8697342627391118 \t Test Score: 0.8918227641571556\n",
      "Fit Time: 240.17977023124695\n",
      "\n",
      "Support Vector Machine\n",
      "----------------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'C': 100, 'kernel': 'rbf'}\n",
      "\n",
      "Best Score: 0.63232451508251 \t Test Score: 0.6520592369914531\n",
      "Fit Time: 42.32074213027954\n",
      "\n",
      "ANN\n",
      "---\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 75, 50, 25), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "\n",
      "Best Score: 0.6173655096010308 \t Test Score: 0.6055873165743998\n",
      "Fit Time: 41.21523594856262\n",
      "\n",
      "KNN\n",
      "---\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'n_neighbors': 15, 'weights': 'distance'}\n",
      "\n",
      "Best Score: 0.8265983259438945 \t Test Score: 0.8534763445583027\n",
      "Fit Time: 0.033991098403930664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "#sample data to 7000 rows\n",
    "sample_testing = df.sample(1000, random_state=42)\n",
    "\n",
    "#take other 7000 for training\n",
    "df = df.drop(sample_testing.index)\n",
    "\n",
    "\n",
    "best_models = []\n",
    "\n",
    "def god_function_7000(dirty_df):\n",
    "    for model in models:\n",
    "        print(model[\"name\"])\n",
    "        print(\"-\"*len(model[\"name\"]))\n",
    "        if model[\"scalable\"] is not None:\n",
    "            if model[\"scalable\"] == 0:\n",
    "                clean_df = pd.DataFrame(cleaning_pipeline.fit_transform(dirty_df))\n",
    "            if model[\"scalable\"] == 1:\n",
    "                clean_df = pd.DataFrame(cleaning_pipeline_scaled.fit_transform(dirty_df))\n",
    "        X = clean_df.drop('label__quality', axis=1)\n",
    "        y = clean_df['label__quality']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "        grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=10, n_jobs=-1)\n",
    "        grid = grid.fit(X_train, y_train)\n",
    "        print(\"Best Parameters:\")\n",
    "        print(grid.best_params_)\n",
    "        print(\"\")\n",
    "        print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "        print(\"Fit Time:\", grid.refit_time_)\n",
    "        print(\"\")\n",
    "        m = {\n",
    "            \"name\": model[\"name\"],\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"best_score\": grid.best_score_,\n",
    "            \"fit_time\": grid.refit_time_,\n",
    "            \"test_score\":  grid.score(X_test, y_test)\n",
    "        }\n",
    "        best_models.append(m)\n",
    "\n",
    "        #save best models to json\n",
    "        with open(\"./models/best_models_7000_samples.json\", \"w\") as f:\n",
    "            json.dump(best_models, f, indent=4)\n",
    "            \n",
    "        #save best estimator from grid with pickle\n",
    "        with open(\"./models/7000_samples/\" + model[\"name\"] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(grid.best_estimator_, f)\n",
    "        \n",
    "\n",
    "god_function_7000(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluating best features based on p-values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Approach to identify best features based on p-values output by the model.\n",
    "\n",
    "Test for Linear Regression and Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def calculate_significant_features(X, y, model):\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "\n",
    "    residuals = y - model.predict(X)\n",
    "\n",
    "    n = len(y)\n",
    "    p = X.shape[1]\n",
    "    df = n - p - 1\n",
    "\n",
    "    mse = np.sum(residuals ** 2) / df\n",
    "    variance_covariance_matrix = mse * np.linalg.inv(np.dot(X.T, X))\n",
    "    standard_errors = np.sqrt(np.diagonal(variance_covariance_matrix))\n",
    "\n",
    "\n",
    "    t_values = coefficients / standard_errors\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df))\n",
    "\n",
    "    headers = ['Feature', 'Coefficient', 'Standard Error', 't-value', 'p-value']\n",
    "\n",
    "    prediction_metrics = pd.DataFrame(columns=headers)\n",
    "    for i in range(len(coefficients)):\n",
    "        prediction_metrics.loc[i] = [X.columns.values[i], coefficients[i], standard_errors[i], t_values[i], p_values[i]]\n",
    "\n",
    "    #remove rows with p-value > 0.05\n",
    "    features_to_remove = prediction_metrics[prediction_metrics['p-value'] > 0.05]['Feature'].values\n",
    "    print(\"Removing features: \", features_to_remove)\n",
    "    prediction_metrics = prediction_metrics[prediction_metrics['p-value'] < 0.05]\n",
    "    return prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "----------------\n",
      "Dropping Features:  ['num__free sulfur dioxide', 'num__calcium', 'num__minerals', 'cat__wine type_Pinot noir', 'num__residual sugar']\n",
      "Best Parameters:\n",
      "{'copy_X': True, 'fit_intercept': True, 'n_jobs': -1}\n",
      "\n",
      "Best Score: 0.5384098923150409 \t Test Score: 0.5304305139944511\n",
      "Fit Time: 0.010992050170898438\n",
      "\n",
      "Removing features:  ['num__citric acid' 'num__flavanoids' 'num__sulphates'\n",
      " 'cat__wine type_Cabernet Sauvignon' 'cat__wine type_Chardonnay']\n",
      "Best Parameters:\n",
      "{'copy_X': True, 'fit_intercept': True, 'n_jobs': -1}\n",
      "\n",
      "Best Score: 0.5386797041044479 \t Test Score: 0.530024283313\n",
      "Fit Time: 0.00799560546875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_models = [{\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"estimator\": LinearRegression(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"fit_intercept\": [True, False],\n",
    "                \"copy_X\": [True, False],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    }\n",
    "    ]\n",
    "dirty_df = df.copy(deep=True)\n",
    "for model in test_models:\n",
    "    print(model[\"name\"])\n",
    "    print(\"-\"*len(model[\"name\"]))\n",
    "    pipeline = cleaning_pipeline\n",
    "    #pipeline.steps.pop(2)\n",
    "    clean_df = pd.DataFrame(pipeline.fit_transform(dirty_df))\n",
    "    X = clean_df.drop('label__quality', axis=1)\n",
    "    y = clean_df['label__quality']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "    grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "    print(\"Fit Time:\", grid.refit_time_)\n",
    "    print(\"\")\n",
    "    best_model = grid.best_estimator_\n",
    "    significant_features = calculate_significant_features(X_train, y_train, best_model)\n",
    "    #keep columns of X only if they are present in significant_features\n",
    "    X_train = X_train[significant_features['Feature'].values]\n",
    "    X_test = X_test[significant_features['Feature'].values]\n",
    "    grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "    print(\"Fit Time:\", grid.refit_time_)\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8ef5791812a9b011d871b4450c5904387dab1ce99926e48021d45ad42ef8d31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
