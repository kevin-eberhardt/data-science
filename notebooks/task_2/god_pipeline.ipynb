{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# God Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data from wines: 8000it [00:00, 17481.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wine type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>minerals</th>\n",
       "      <th>calcium</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pinot noir</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.1</td>\n",
       "      <td>76.729301</td>\n",
       "      <td>894.94</td>\n",
       "      <td>186.639301</td>\n",
       "      <td>109.91</td>\n",
       "      <td>0.048</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.32</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.795712</td>\n",
       "      <td>1160.95</td>\n",
       "      <td>251.875712</td>\n",
       "      <td>247.08</td>\n",
       "      <td>0.039</td>\n",
       "      <td>15.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.99163</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.5</td>\n",
       "      <td>85.193710</td>\n",
       "      <td>789.82</td>\n",
       "      <td>304.703710</td>\n",
       "      <td>219.51</td>\n",
       "      <td>0.035</td>\n",
       "      <td>45.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.98949</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>12.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.27</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.976525</td>\n",
       "      <td>777.86</td>\n",
       "      <td>237.586525</td>\n",
       "      <td>225.61</td>\n",
       "      <td>0.045</td>\n",
       "      <td>48.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1.00014</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.19</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.599673</td>\n",
       "      <td>785.72</td>\n",
       "      <td>95.399673</td>\n",
       "      <td>89.80</td>\n",
       "      <td>0.041</td>\n",
       "      <td>62.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.99508</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.37</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.3</td>\n",
       "      <td>22.403749</td>\n",
       "      <td>1044.95</td>\n",
       "      <td>289.523749</td>\n",
       "      <td>267.12</td>\n",
       "      <td>0.057</td>\n",
       "      <td>25.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.99480</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Merlot</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.5</td>\n",
       "      <td>23.875866</td>\n",
       "      <td>888.61</td>\n",
       "      <td>133.545866</td>\n",
       "      <td>109.67</td>\n",
       "      <td>0.047</td>\n",
       "      <td>20.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.99178</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.48</td>\n",
       "      <td>11.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chardonnay</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.7</td>\n",
       "      <td>23.309699</td>\n",
       "      <td>1381.79</td>\n",
       "      <td>266.529699</td>\n",
       "      <td>243.22</td>\n",
       "      <td>0.052</td>\n",
       "      <td>56.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.99398</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4.7</td>\n",
       "      <td>49.165745</td>\n",
       "      <td>1456.41</td>\n",
       "      <td>269.915745</td>\n",
       "      <td>220.75</td>\n",
       "      <td>0.046</td>\n",
       "      <td>57.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gamay</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>8.9</td>\n",
       "      <td>54.450579</td>\n",
       "      <td>929.44</td>\n",
       "      <td>377.690579</td>\n",
       "      <td>323.24</td>\n",
       "      <td>0.036</td>\n",
       "      <td>8.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.99350</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            wine type  fixed acidity  volatile acidity  citric acid  \\\n",
       "0          Pinot noir            5.8              0.15         0.49   \n",
       "1              Merlot            6.6              0.25         0.32   \n",
       "2          Chardonnay            6.7              0.21         0.34   \n",
       "3              Merlot            8.3              0.28         0.27   \n",
       "4              Merlot            7.5              0.42         0.19   \n",
       "5              Merlot            7.3              0.34         0.30   \n",
       "6              Merlot            7.6              0.21         0.49   \n",
       "7          Chardonnay            6.0              0.25         0.40   \n",
       "8  Cabernet Sauvignon            6.7              0.18         0.19   \n",
       "9               Gamay            7.7              0.28         0.39   \n",
       "\n",
       "   residual sugar  magnesium  flavanoids    minerals  calcium  chlorides  \\\n",
       "0             1.1  76.729301      894.94  186.639301   109.91      0.048   \n",
       "1             5.6   4.795712     1160.95  251.875712   247.08      0.039   \n",
       "2             1.5  85.193710      789.82  304.703710   219.51      0.035   \n",
       "3            17.5  11.976525      777.86  237.586525   225.61      0.045   \n",
       "4             6.9   5.599673      785.72   95.399673    89.80      0.041   \n",
       "5             1.3  22.403749     1044.95  289.523749   267.12      0.057   \n",
       "6             2.5  23.875866      888.61  133.545866   109.67      0.047   \n",
       "7             5.7  23.309699     1381.79  266.529699   243.22      0.052   \n",
       "8             4.7  49.165745     1456.41  269.915745   220.75      0.046   \n",
       "9             8.9  54.450579      929.44  377.690579   323.24      0.036   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 21.0                  98.0  0.99290  3.19       0.48   \n",
       "1                 15.0                  68.0  0.99163  2.96       0.52   \n",
       "2                 45.0                 123.0  0.98949  3.24       0.36   \n",
       "3                 48.0                 253.0  1.00014  3.02       0.56   \n",
       "4                 62.0                 150.0  0.99508  3.23       0.37   \n",
       "5                 25.0                 173.0  0.99480  3.26       0.51   \n",
       "6                 20.0                 130.0  0.99178  3.15       0.48   \n",
       "7                 56.0                 152.0  0.99398  3.16       0.88   \n",
       "8                 57.0                 161.0  0.99460  3.32       0.66   \n",
       "9                  8.0                 117.0  0.99350  3.06       0.38   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.2        5  \n",
       "1     11.1        6  \n",
       "2     12.6        7  \n",
       "3      9.1        6  \n",
       "4     10.0        6  \n",
       "5      9.1        6  \n",
       "6     11.1        5  \n",
       "7     10.5        6  \n",
       "8     10.5        6  \n",
       "9     12.0        2  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#1. load environment variables and data\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#add working directory to sys path to execute utils/dataset.py\n",
    "working_dir = os.environ.get(\"WORKING_DIRECTORY\")\n",
    "sys.path.insert(0, working_dir)\n",
    "\n",
    "from utils.dataset import get_data \n",
    "\n",
    "from utils.pipeline_moduls import fs_colinearity, fs_vif, outlier_label, outlier_num, dim_reduction\n",
    "\n",
    "df = get_data()\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "#drop 'quality' from numerical features (its a series)\n",
    "numerical_features = numerical_features.drop('quality')\n",
    "label = pd.Series('quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection_label = FunctionTransformer(outlier_label).set_output(transform=\"pandas\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection = FunctionTransformer(outlier_num).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def feature_selection(df,colinearity_threshold=0.5, correlation_threshold=0.1, vif_threshold=5):\n",
    "    dropped_features = []\n",
    "    dropped_features_set = set(dropped_features)\n",
    "\n",
    "    # Add elements from fs_colinearity to dropped_features_set\n",
    "    dropped_features_set.update(fs_colinearity(df, colinearity_threshold, correlation_threshold))\n",
    "\n",
    "    # Add elements from fs_vif to dropped_features_set\n",
    "    dropped_features_set.update(fs_vif(df, correlation_threshold, vif_threshold))\n",
    "\n",
    "    # Convert dropped_features_set back to a list\n",
    "    dropped_features = list(dropped_features_set)\n",
    "      # Save dropped features list to a JSON file\n",
    "    with open('dropped_features.json', 'w') as f:\n",
    "        json.dump(dropped_features, f)\n",
    "    return df\n",
    "feature_selection = FunctionTransformer(feature_selection).set_output(transform=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline = Pipeline(steps=[\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Pipeline: Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\").set_output(transform=\"pandas\")\n",
    "\n",
    "#pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[])\n",
    "categorical_pipeline.steps.append(('imputer', categorical_imputer))\n",
    "categorical_pipeline.steps.append(('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform=\"pandas\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Pipeline: Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for numerical features\n",
    "numeric_pipeline = Pipeline(steps=[])\n",
    "numerical_imputer = SimpleImputer(strategy=\"mean\").set_output(transform=\"pandas\")\n",
    "\n",
    "numeric_pipeline.steps.append(('imputer', numerical_imputer))\n",
    "numeric_pipeline.steps.append(('outlier_detection', outlier_detection))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Pipeline: Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for label\n",
    "label_pipeline = Pipeline(steps=[])\n",
    "label_pipeline.steps.append(('imputer', numerical_imputer))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('label', label_pipeline, label)\n",
    "    ]).set_output(transform=\"pandas\")\n",
    "cleaning_pipeline.steps.append(('preprocessor', preprocessor))\n",
    "cleaning_pipeline.steps.append((\"outlier_detection_label\", outlier_detection_label))\n",
    "cleaning_pipeline.steps.append(('feature_selection', feature_selection))\n",
    "cleaning_pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"estimator\": LinearRegression(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"fit_intercept\": [True, False],\n",
    "                \"copy_X\": [True, False],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTreeRegressor\",\n",
    "        \"estimator\": DecisionTreeRegressor(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"criterion\": [\"squared_error\"],\n",
    "                \"splitter\": [\"best\", \"random\"],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 5, 10]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForestRegressor\",\n",
    "        \"estimator\": RandomForestRegressor(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"criterion\": [\"squared_error\"],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 5, 10],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gradient Boosting Regressor\",\n",
    "        \"estimator\": GradientBoostingRegressor(),\n",
    "        \"hyperparameters\":\n",
    "        {\n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"max_depth\": [None, 2, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"learning_rate\": [0.01, 0.011, 0.012],\n",
    "                \"loss\": [\"squared_error\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Support Vector Machine\",\n",
    "        \"estimator\": SVR(),\n",
    "        \"hyperparameters\":\n",
    "        {\n",
    "            \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"degree\": [1, 2, 3, 4, 5],\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 1000],\n",
    "            \"epsilon\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "best_models_runtime = []\n",
    "\n",
    "def god_function(dirty_df):\n",
    "    for model in models:\n",
    "        print(model[\"name\"])\n",
    "        print(\"-\"*len(model[\"name\"]))\n",
    "        clean_df = pd.DataFrame(cleaning_pipeline.fit_transform(dirty_df))\n",
    "        X = clean_df.drop('label__quality', axis=1)\n",
    "        y = clean_df['label__quality']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "        grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "        grid = grid.fit(X_train, y_train)\n",
    "        print(\"Best Parameters:\")\n",
    "        print(grid.best_params_)\n",
    "        print(\"\")\n",
    "        print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "        print(\"Fit Time:\", grid.refit_time_)\n",
    "        print(\"\")\n",
    "        #save best estimator from grid with pickle\n",
    "        with open(\"./models/\" + model[\"name\"] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(grid.best_estimator_, f)\n",
    "        \n",
    "        #save best model runtime\n",
    "        best_models_runtime.append({\n",
    "            \"name\": model[\"name\"], \n",
    "            \"runtime\": grid.refit_time_,\n",
    "            \"best_score\": grid.best_score_,\n",
    "            \"best_params\": grid.best_params_\n",
    "        })\n",
    "\n",
    "god_function(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significant_features(X, y, model):\n",
    "    coefficients = model.coef_\n",
    "    intercept = model.intercept_\n",
    "\n",
    "\n",
    "    residuals = y - model.predict(X)\n",
    "\n",
    "    n = len(y)\n",
    "    p = X.shape[1]\n",
    "    df = n - p - 1\n",
    "\n",
    "    mse = np.sum(residuals ** 2) / df\n",
    "    variance_covariance_matrix = mse * np.linalg.inv(np.dot(X.T, X))\n",
    "    standard_errors = np.sqrt(np.diagonal(variance_covariance_matrix))\n",
    "\n",
    "\n",
    "    t_values = coefficients / standard_errors\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df))\n",
    "\n",
    "    headers = ['Feature', 'Coefficient', 'Standard Error', 't-value', 'p-value']\n",
    "\n",
    "    prediction_metrics = pd.DataFrame(columns=headers)\n",
    "    for i in range(len(coefficients)):\n",
    "        prediction_metrics.loc[i] = [X.columns.values[i], coefficients[i], standard_errors[i], t_values[i], p_values[i]]\n",
    "\n",
    "    #remove rows with p-value > 0.05\n",
    "    features_to_remove = prediction_metrics[prediction_metrics['p-value'] > 0.05]['Feature'].values\n",
    "    print(\"Removing features: \", features_to_remove)\n",
    "    prediction_metrics = prediction_metrics[prediction_metrics['p-value'] < 0.05]\n",
    "    return prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "----------------\n",
      "Best Parameters:\n",
      "{'copy_X': True, 'fit_intercept': False, 'n_jobs': -1}\n",
      "\n",
      "Best Score: 0.5536345285850655 \t Test Score: 0.5586981830127015\n",
      "Fit Time: 0.007969141006469727\n",
      "\n",
      "Removing features:  ['num__citric acid' 'num__magnesium' 'num__flavanoids' 'num__minerals'\n",
      " 'num__calcium' 'num__chlorides' 'num__total sulfur dioxide']\n",
      "Best Parameters:\n",
      "{'copy_X': True, 'fit_intercept': False, 'n_jobs': -1}\n",
      "\n",
      "Best Score: 0.5534053988619101 \t Test Score: 0.5563535229323042\n",
      "Fit Time: 0.004335880279541016\n",
      "\n",
      "DecisionTreeRegressor\n",
      "---------------------\n",
      "Best Parameters:\n",
      "{'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\n",
      "Best Score: 0.7421017128377512 \t Test Score: 0.8814416670502254\n",
      "Fit Time: 0.10364890098571777\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeRegressor' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m best_model \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbest_estimator_\n\u001b[0;32m---> 43\u001b[0m significant_features \u001b[39m=\u001b[39m calculate_significant_features(X_train, y_train, best_model)\n\u001b[1;32m     44\u001b[0m \u001b[39m#keep columns of X only if they are present in significant_features\u001b[39;00m\n\u001b[1;32m     45\u001b[0m X_train \u001b[39m=\u001b[39m X_train[significant_features[\u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues]\n",
      "Cell \u001b[0;32mIn[113], line 2\u001b[0m, in \u001b[0;36mcalculate_significant_features\u001b[0;34m(X, y, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_significant_features\u001b[39m(X, y, model):\n\u001b[0;32m----> 2\u001b[0m     coefficients \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcoef_\n\u001b[1;32m      3\u001b[0m     intercept \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m      6\u001b[0m     residuals \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39m model\u001b[39m.\u001b[39mpredict(X)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeRegressor' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "test_models = [{\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"estimator\": LinearRegression(),\n",
    "        \"hyperparameters\":\n",
    "            {\n",
    "                \"fit_intercept\": [True, False],\n",
    "                \"copy_X\": [True, False],\n",
    "                \"n_jobs\": [-1]\n",
    "            }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Support Vector Machine\",\n",
    "        \"estimator\": SVR(),\n",
    "        \"hyperparameters\":\n",
    "        {\n",
    "            \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "            \"degree\": [1, 2, 3, 4, 5],\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "            \"C\": [0.1, 1, 10, 100, 1000],\n",
    "            \"epsilon\": [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    }\n",
    "    }\n",
    "    ]\n",
    "dirty_df = df.copy(deep=True)\n",
    "for model in test_models:\n",
    "    print(model[\"name\"])\n",
    "    print(\"-\"*len(model[\"name\"]))\n",
    "    pipeline = cleaning_pipeline\n",
    "    #pipeline.steps.pop(2)\n",
    "    clean_df = pd.DataFrame(pipeline.fit_transform(dirty_df))\n",
    "    X = clean_df.drop('label__quality', axis=1)\n",
    "    y = clean_df['label__quality']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "    grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "    print(\"Fit Time:\", grid.refit_time_)\n",
    "    print(\"\")\n",
    "    best_model = grid.best_estimator_\n",
    "    significant_features = calculate_significant_features(X_train, y_train, best_model)\n",
    "    #keep columns of X only if they are present in significant_features\n",
    "    X_train = X_train[significant_features['Feature'].values]\n",
    "    X_test = X_test[significant_features['Feature'].values]\n",
    "    grid = GridSearchCV(model[\"estimator\"], model[\"hyperparameters\"], cv=5, n_jobs=-1)\n",
    "    grid = grid.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", grid.best_score_, \"\\t\", \"Test Score:\", grid.score(X_test, y_test))\n",
    "    print(\"Fit Time:\", grid.refit_time_)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing features:  ['num__citric acid' 'num__magnesium' 'num__flavanoids' 'num__minerals'\n",
      " 'num__calcium' 'num__chlorides' 'num__total sulfur dioxide']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>t-value</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num__fixed acidity</td>\n",
       "      <td>0.103786</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>5.849269</td>\n",
       "      <td>5.181988e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num__volatile acidity</td>\n",
       "      <td>-1.893428</td>\n",
       "      <td>0.126769</td>\n",
       "      <td>-14.936076</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num__residual sugar</td>\n",
       "      <td>0.088170</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>14.907957</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num__free sulfur dioxide</td>\n",
       "      <td>0.006213</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>7.805723</td>\n",
       "      <td>6.883383e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>num__density</td>\n",
       "      <td>-183.998630</td>\n",
       "      <td>15.936008</td>\n",
       "      <td>-11.546093</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>num__pH</td>\n",
       "      <td>0.850246</td>\n",
       "      <td>0.086788</td>\n",
       "      <td>9.796794</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>num__sulphates</td>\n",
       "      <td>0.621911</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>6.190640</td>\n",
       "      <td>6.364433e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>num__alcohol</td>\n",
       "      <td>0.134464</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>6.373369</td>\n",
       "      <td>1.978397e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cat__wine type_Cabernet Sauvignon</td>\n",
       "      <td>183.454918</td>\n",
       "      <td>15.762737</td>\n",
       "      <td>11.638519</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cat__wine type_Chardonnay</td>\n",
       "      <td>183.473879</td>\n",
       "      <td>15.762451</td>\n",
       "      <td>11.639933</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cat__wine type_Gamay</td>\n",
       "      <td>179.123066</td>\n",
       "      <td>15.762301</td>\n",
       "      <td>11.364018</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cat__wine type_Merlot</td>\n",
       "      <td>183.473916</td>\n",
       "      <td>15.762450</td>\n",
       "      <td>11.639937</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cat__wine type_Pinot noir</td>\n",
       "      <td>183.441502</td>\n",
       "      <td>15.762825</td>\n",
       "      <td>11.637603</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Feature  Coefficient  Standard Error    t-value  \\\n",
       "0                  num__fixed acidity     0.103786        0.017743   5.849269   \n",
       "1               num__volatile acidity    -1.893428        0.126769 -14.936076   \n",
       "3                 num__residual sugar     0.088170        0.005914  14.907957   \n",
       "9            num__free sulfur dioxide     0.006213        0.000796   7.805723   \n",
       "11                       num__density  -183.998630       15.936008 -11.546093   \n",
       "12                            num__pH     0.850246        0.086788   9.796794   \n",
       "13                     num__sulphates     0.621911        0.100460   6.190640   \n",
       "14                       num__alcohol     0.134464        0.021098   6.373369   \n",
       "15  cat__wine type_Cabernet Sauvignon   183.454918       15.762737  11.638519   \n",
       "16          cat__wine type_Chardonnay   183.473879       15.762451  11.639933   \n",
       "17               cat__wine type_Gamay   179.123066       15.762301  11.364018   \n",
       "18              cat__wine type_Merlot   183.473916       15.762450  11.639937   \n",
       "19          cat__wine type_Pinot noir   183.441502       15.762825  11.637603   \n",
       "\n",
       "         p-value  \n",
       "0   5.181988e-09  \n",
       "1   0.000000e+00  \n",
       "3   0.000000e+00  \n",
       "9   6.883383e-15  \n",
       "11  0.000000e+00  \n",
       "12  0.000000e+00  \n",
       "13  6.364433e-10  \n",
       "14  1.978397e-10  \n",
       "15  0.000000e+00  \n",
       "16  0.000000e+00  \n",
       "17  0.000000e+00  \n",
       "18  0.000000e+00  \n",
       "19  0.000000e+00  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END TESTING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Meth-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model with pickle\n",
    "best_model = pickle.load(open('best_model__random_forest.pkl','rb'))\n",
    "#select randomly 1000 data points from df and drop selected ones\n",
    "df_validation = df.sample(n=1000, random_state=1)\n",
    "clean_df = df.drop(df_validation.index)\n",
    "\n",
    "X = clean_df.drop('label__quality', axis=1)\n",
    "y = clean_df['label__quality']\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=200, stratify=y)\n",
    "\n",
    "print(best_model.score(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df.sample(n=1000, random_state=42)\n",
    "\n",
    "clean_df = df.drop(df_validation.index)\n",
    "clean_df = pd.DataFrame(cleaning_pipeline.fit_transform(clean_df))\n",
    "\n",
    "X_clean = clean_df.drop('label__quality', axis=1)\n",
    "y_clean = clean_df['label__quality']\n",
    "\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, y_clean, test_size=0.2, random_state=200, stratify=y_clean)\n",
    "best_model = RandomForestRegressor(criterion = 'squared_error', max_depth = None, min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200)\n",
    "best_model = best_model.fit(X_train_clean, y_train_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.score(X_test_clean, y_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.DataFrame(cleaning_pipeline.fit_transform(df_validation))\n",
    "X_validation = df_validation.drop('label__quality', axis=1)\n",
    "y_validation = df_validation['label__quality']\n",
    "\n",
    "best_model.score(X_validation, y_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
